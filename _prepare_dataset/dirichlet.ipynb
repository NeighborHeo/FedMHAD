{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suncheol/code/FedTest/.venv/lib/python3.8/site-packages/albumentations/augmentations/transforms.py:1175: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://poutyne.org/examples/semantic_segmentation.html\n",
    "# \n",
    "from comet_ml import Experiment\n",
    "from comet_ml.integration.pytorch import log_model\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchmetrics\n",
    "\n",
    "# Poutyne Model on GPU\n",
    "from poutyne import Model\n",
    "\n",
    "# Custom modules\n",
    "import utils2\n",
    "import utils\n",
    "import datasets\n",
    "import networks\n",
    "import callbacks\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.join(pathlib.Path(\".\").parent.absolute(), '.env_comet'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/neighborheo/test-segmentation/2dec92f882d445c392f1f5fca1bc0a46\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : voc2012-segformer\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed) : 1 (1.03 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/neighborheo/test-segmentation/2594abd1eb2d4fb7ad305144d0843bc5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils.set_seeds(42)\n",
    "\n",
    "def init_arguments():\n",
    "    parser = argparse.ArgumentParser(description='test for segmentation')\n",
    "    parser.add_argument('--model_name', type=str, default='segformer', help='model name (default: segformer)')\n",
    "    parser.add_argument('--dataset', type=str, default='voc2012', help='dataset (default: voc2012)')\n",
    "    parser.add_argument('--data_path', type=str, default='~/.data', help='data path (default: data)')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='input batch size for training (default: 32)')\n",
    "    parser.add_argument('--epochs', type=int, default=100, help='number of epochs to train (default: 100)')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.0005, help='learning rate (default: 0.0005)')\n",
    "    parser.add_argument('--image_size', type=int, default=224, help='image size (default: 224)')\n",
    "    parser.add_argument('--out_image_size', type=int, default=56, help='number of workers (default: 2)')\n",
    "    parser.add_argument('--num_classes', type=int, default=22, help='number of classes (default: 22)')\n",
    "    parser.add_argument('--continue_training', action='store_true', help='continue training (default: False)')\n",
    "    parser.add_argument('--dirichlet_alpha', type=float, default=1.0, help='dirichlet alpha (default: 1.0)')\n",
    "    parser.add_argument('--num_clients', type=int, default=10, help='number of clients (default: 10)')\n",
    "    args = parser.parse_args(\"\")\n",
    "    return args\n",
    "\n",
    "def init_experiment(dataset, model_name):\n",
    "    experiment = Experiment(\n",
    "        api_key = os.getenv('COMET_API_TOKEN'),\n",
    "        project_name = os.getenv('COMET_PROJECT_NAME'),\n",
    "        workspace= os.getenv('COMET_WORKSPACE'),\n",
    "    )\n",
    "    experiment.add_tag(dataset)\n",
    "    experiment.add_tag(model_name)\n",
    "    experiment.set_name(f\"{dataset}-{model_name}\")\n",
    "    return experiment\n",
    "\n",
    "args = init_arguments()\n",
    "experiment = init_experiment(args.dataset, args.model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = datasets.getVOCSegDatasets(output_size=None)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = valid_loader\n",
    "\n",
    "# Creating saving directory\n",
    "save_path = f'saves/{args.model_name}-{args.dataset}-comet_test'\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "def get_labels(filepath):\n",
    "    arr = np.array(Image.open(filepath))\n",
    "    arr[arr>20] = 0\n",
    "    unique_list = np.unique(arr)\n",
    "    unique_list = unique_list[unique_list!=0]\n",
    "    # sorting\n",
    "    unique_list.sort()\n",
    "    return list(unique_list)\n",
    "\n",
    "labels = [get_labels(filepath) for filepath in train_dataset.masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "party_id: 0, num of samples: 137\n",
      "party_id: 1, num of samples: 140\n",
      "party_id: 2, num of samples: 53\n",
      "party_id: 3, num of samples: 130\n",
      "party_id: 4, num of samples: 129\n",
      "party_id: 5, num of samples: 118\n",
      "party_id: 6, num of samples: 117\n",
      "party_id: 7, num of samples: 68\n",
      "party_id: 8, num of samples: 153\n",
      "party_id: 9, num of samples: 82\n"
     ]
    }
   ],
   "source": [
    "def create_label_to_id_map(labels):\n",
    "    label_to_id = {}\n",
    "    index = 0\n",
    "    for label in labels:\n",
    "        label = frozenset(label)\n",
    "        if label not in label_to_id:\n",
    "            label_to_id[label] = index\n",
    "            index += 1\n",
    "    return label_to_id\n",
    "\n",
    "def convert_id_to_label_map(label_to_id):\n",
    "    return {v: k for k, v in label_to_id.items()}\n",
    "\n",
    "def convert_labels_to_ids(labels, label_to_id):\n",
    "    return [label_to_id[frozenset(label)] for label in labels]\n",
    "\n",
    "label_to_id = create_label_to_id_map(labels)\n",
    "id_to_label = convert_id_to_label_map(label_to_id)\n",
    "\n",
    "label_ids = convert_labels_to_ids(labels, label_to_id)\n",
    "\n",
    "N_class = len(label_to_id)\n",
    "N_parties = args.num_clients\n",
    "y_data = label_ids\n",
    "alpha = args.dirichlet_alpha\n",
    "utils2.set_random_seed(42)\n",
    "dirichlet_count = utils2.get_dirichlet_distribution_count(N_class, N_parties, y_data, alpha)\n",
    "split_dirichlet_data_index_dict = utils2.get_split_data_index(y_data, dirichlet_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dirichlet_data_index_dict\n",
    "project_dir = pathlib.Path(\".\").parent.absolute()\n",
    "split_path = project_dir / \"splitfile\" / f\"{args.dataset}\" \n",
    "split_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save split files\n",
    "import json\n",
    "with open(split_path / f'dirichlet_{args.dirichlet_alpha}_for_{args.num_clients}_clients', \"w\") as f:\n",
    "    json.dump(split_dirichlet_data_index_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "# load split files\n",
    "with open(split_path / f'dirichlet_{args.dirichlet_alpha}_for_{args.num_clients}_clients', \"r\") as f:\n",
    "    split_dirichlet_data_index_dict = json.load(f)\n",
    "    \n",
    "# subset of train dataset\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, split_dirichlet_data_index_dict[\"2\"])\n",
    "print(len(train_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
