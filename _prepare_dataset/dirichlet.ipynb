{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://poutyne.org/examples/semantic_segmentation.html\n",
    "# \n",
    "from comet_ml import Experiment\n",
    "from comet_ml.integration.pytorch import log_model\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/suncheol/code/FedTest/0_FedMHAD_Seg\")\n",
    "import os\n",
    "import pathlib\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchmetrics\n",
    "\n",
    "# Poutyne Model on GPU\n",
    "from poutyne import Model\n",
    "\n",
    "# # Custom modules\n",
    "# import utils2\n",
    "import utils\n",
    "import datasets\n",
    "# import networks\n",
    "# import callbacks\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.join(pathlib.Path(\".\").absolute().parent, '.env_comet'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.set_random_seed(42)\n",
    "\n",
    "def init_arguments():\n",
    "    parser = argparse.ArgumentParser(description='test for segmentation')\n",
    "    parser.add_argument('--model_name', type=str, default='segformer', help='model name (default: segformer)')\n",
    "    parser.add_argument('--dataset', type=str, default='voc2012', help='dataset (default: voc2012)')\n",
    "    parser.add_argument('--data_path', type=str, default='~/.data', help='data path (default: data)')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='input batch size for training (default: 32)')\n",
    "    parser.add_argument('--epochs', type=int, default=100, help='number of epochs to train (default: 100)')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.0005, help='learning rate (default: 0.0005)')\n",
    "    parser.add_argument('--image_size', type=int, default=224, help='image size (default: 224)')\n",
    "    parser.add_argument('--out_image_size', type=int, default=56, help='number of workers (default: 2)')\n",
    "    parser.add_argument('--num_classes', type=int, default=3, help='number of classes (default: 22)')\n",
    "    parser.add_argument('--continue_training', action='store_true', help='continue training (default: False)')\n",
    "    parser.add_argument('--dirichlet_alpha', type=float, default=0.1, help='dirichlet alpha (default: 1.0)')\n",
    "    parser.add_argument('--num_clients', type=int, default=3, help='number of clients (default: 10)')\n",
    "    parser.add_argument('--malicious', type=int, default=0, help='number of malicious clients (default: 0)')\n",
    "    args = parser.parse_args(\"\")\n",
    "    return args\n",
    "\n",
    "def init_experiment(dataset, model_name):\n",
    "    experiment = Experiment(\n",
    "        api_key = os.getenv('COMET_API_TOKEN'),\n",
    "        project_name = os.getenv('COMET_PROJECT_NAME'),\n",
    "        workspace= os.getenv('COMET_WORKSPACE'),\n",
    "    )\n",
    "    experiment.add_tag(dataset)\n",
    "    experiment.add_tag(model_name)\n",
    "    experiment.set_name(f\"{dataset}-{model_name}\")\n",
    "    return experiment\n",
    "\n",
    "args = init_arguments()\n",
    "experiment = init_experiment(args.dataset, args.model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "datasetpartition = datasets.PascalVocSegmentationPartition(args)\n",
    "train_dataset, valid_dataset = datasetpartition.load_partition(-1)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0 )\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0 )\n",
    "test_loader = valid_loader\n",
    "\n",
    "# Creating saving directory\n",
    "save_path = f'saves/{args.model_name}-{args.dataset}-comet_test'\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pascal voc dataset :\n",
    "class_names = ['background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car',\n",
    "                'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
    "                'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "def get_labels(filepath):\n",
    "    arr = np.array(Image.open(filepath))\n",
    "    arr[arr>20] = 0\n",
    "    # for i in range(1, 21):\n",
    "    #     # only use 1, 9, 12\n",
    "    #     if i not in [1, 9, 12]:\n",
    "    #         arr[arr==i] = 0\n",
    "    unique_list = np.unique(arr)\n",
    "    unique_list = unique_list[unique_list!=0]\n",
    "    # sorting\n",
    "    unique_list.sort()\n",
    "    return list(unique_list)\n",
    "\n",
    "labels = [get_labels(filepath) for filepath in train_dataset.masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = [get_labels(filepath) for filepath in valid_dataset.masks]\n",
    "test_not_empty_indices = [i for i, label in enumerate(test_labels) if len(label) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_not_empty_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_to_id_map(labels):\n",
    "    label_to_id = {}\n",
    "    index = 0\n",
    "    for label in labels:\n",
    "        label = frozenset(label)\n",
    "        if label not in label_to_id:\n",
    "            label_to_id[label] = index\n",
    "            index += 1\n",
    "    return label_to_id\n",
    "\n",
    "def convert_id_to_label_map(label_to_id):\n",
    "    return {v: k for k, v in label_to_id.items()}\n",
    "\n",
    "def convert_labels_to_ids(labels, label_to_id):\n",
    "    return [label_to_id[frozenset(label)] for label in labels]\n",
    "\n",
    "label_to_id = create_label_to_id_map(labels)\n",
    "id_to_label = convert_id_to_label_map(label_to_id)\n",
    "\n",
    "label_ids = convert_labels_to_ids(labels, label_to_id)\n",
    "\n",
    "N_class = len(label_to_id)\n",
    "N_parties = args.num_clients\n",
    "y_data = label_ids\n",
    "dirichlet_count = utils.get_dirichlet_distribution_count(N_class, N_parties, y_data, args.dirichlet_alpha)\n",
    "split_dirichlet_data_index_dict = utils.get_split_data_index(y_data, dirichlet_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = {}\n",
    "for i in range(N_parties):\n",
    "    index_list = split_dirichlet_data_index_dict[i]\n",
    "    index = []\n",
    "    for k in index_list:\n",
    "        if len(labels[k]) != 0:\n",
    "            index.append(k)\n",
    "    indices[i] = index\n",
    "    print(f\"party {len(indices)}, index : {len(index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = pathlib.Path(\".\").absolute().parent\n",
    "split_path = project_dir / \"splitfile\" / f\"{args.dataset}_{args.num_clients}_clients\" \n",
    "split_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save split files\n",
    "import json\n",
    "with open(split_path / f'dirichlet_{args.dirichlet_alpha}_for_{args.num_clients}_clients', \"w\") as f:\n",
    "    json.dump(indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load split files\n",
    "with open(split_path / f'dirichlet_{args.dirichlet_alpha}_for_{args.num_clients}_clients', \"r\") as f:\n",
    "    split_dirichlet_data_index_dict = json.load(f)\n",
    "    \n",
    "# subset of train dataset\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, split_dirichlet_data_index_dict[\"2\"])\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
