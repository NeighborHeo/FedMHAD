{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"./IID_Performance.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seaborn을 사용하여 bar plot을 그립니다.\n",
    "# sns.barplot(x='index', y='mIOU (Test)', data=df)\n",
    "sns.barplot(data=df.transpose())\n",
    "# # 그래프에 제목을 추가합니다.\n",
    "# plt.title('Bar Plot')\n",
    "\n",
    "# # 그래프를 표시합니다.\n",
    "# plt.show()\n",
    "# # Plot\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "# sns.set_context(\"paper\", font_scale=1.5)\n",
    "# fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# # ax = sns.barplot(x=df.index, y=[\"mIOU (Test)\", \"F1 (Test)\", \"Acc (Test)\"], data=df, palette=\"Blues_d\")\n",
    "# ax = sns.barplot(x=df.index, y=[\"mIOU (Test)\", \"F1 (Test)\", \"Acc (Test)\"], data=df, palette=\"Blues_d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 데이터\n",
    "data = {\n",
    "    'clients': ['client1', 'client2', 'client3', 'client4', 'client5', 'client6', 'client7', 'client8', 'client9', 'client10'],\n",
    "    'mIOU': [0.1, 0.5, 0.7, 0.3, 0.6, 0.8, 0.5, 0.4, 0.6, 0.7],\n",
    "    'F1': [0.5, 0.6, 0.7, 0.8, 0.5, 0.4, 0.6, 0.7, 0.5, 0.6],\n",
    "    'ACC': [0.9, 0.8, 0.7, 0.6, 0.7, 0.8, 0.9, 0.7, 0.8, 0.6]\n",
    "}\n",
    "\n",
    "# DataFrame으로 변환\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bargraph(df, save_path, title):\n",
    "    \"\"\"\n",
    "    :param df: DataFrame with columns 'clients', 'mIOU', 'F1', 'Acc'\n",
    "    \"\"\"\n",
    "    # 데이터를 긴 형식으로 변경합니다.\n",
    "    df_long = pd.melt(df, id_vars=['clients'], value_vars=df.columns[1:])\n",
    "    \n",
    "    # 바 그래프 그리기\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x='clients', y='value', hue='variable', data=df_long)\n",
    "    \n",
    "    # x축과 y축 라벨 설정\n",
    "    # plt.xlabel('Clients')\n",
    "    plt.ylabel('Scores')\n",
    "    \n",
    "    # 그래프 제목 설정\n",
    "    plt.title(title)\n",
    "    \n",
    "    # 범례 추가\n",
    "    plt.legend(title='Metrics (Test)')\n",
    "    \n",
    "    # 그래프 표시\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./NIID_Performance.csv\", index_col=0)\n",
    "df.reset_index(names='clients', inplace=True)\n",
    "df.columns = df.columns.str.replace(' (Test)','')\n",
    "df.drop(columns=['mIOU', 'loss'], inplace=True)\n",
    "print(df.columns)\n",
    "plot_bargraph(df, './NIID_Performance.png', 'Segmentation Performance on NIID Dataset')\n",
    "df = pd.read_csv(\"./IID_Performance.csv\", index_col=0)\n",
    "df.reset_index(names='clients', inplace=True)\n",
    "df.columns = df.columns.str.replace(' (Test)','')\n",
    "df.drop(columns=['mIOU', 'loss'], inplace=True)\n",
    "print(df.columns)\n",
    "plot_bargraph(df, './IID_Performance.png', 'Segmentation Performance on IID Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 각각의 Client로컬 학습 모델의 Receptive Field를 계산한다.\n",
    "# \n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "import torch\n",
    "import models\n",
    "import utils\n",
    "\n",
    "def load_all_models(path):\n",
    "    folder_path = path\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pth\"):\n",
    "                file_list.append(pathlib.Path(root) / file)\n",
    "    # sorting file_list\n",
    "    file_list.sort()\n",
    "    # dirnames\n",
    "    model_state_dict = {}\n",
    "    for file in file_list:\n",
    "        print(\"file_name : \", file)\n",
    "        # get index from file name ex. \"client_0_best_models\" > \"0\", \"test 1\" > \"1\"\n",
    "        if 'global' in file.parent.name:\n",
    "            model_state_dict[-1] = torch.load(file) \n",
    "            print(\"file.parent.name: \", file.parent.name)\n",
    "        else:\n",
    "            index = int(file.parent.name.split('_')[1])\n",
    "            model_state_dict[index] = torch.load(file)\n",
    "            print(\"file.parent.name: \", file.parent.name, \"index: \", index)\n",
    "    return model_state_dict\n",
    "\n",
    "model_state_dict = load_all_models(\"/home/suncheol/code/FedTest/0_FedMHAD_Seg/checkpoints/_56121\")# 55227\")\n",
    "model_state_dict[-1] = torch.load(\"/home/suncheol/code/FedTest/0_FedMHAD_Seg/checkpoints/38125/global/model_round99_acc82.04_loss0.64.pth\")\n",
    "model_state_dict[-2] = torch.load(\"/home/suncheol/code/FedTest/0_FedMHAD_Seg/checkpoints/42301/global/model_round99_acc83.74_loss0.58.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import config\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "args = config.init_args(server=False, ipykernel=True)\n",
    "args.output_size = 224\n",
    "datasetpartition = datasets.PascalVocSegmentationPartition(args)\n",
    "train_dataset, valid_dataset = datasetpartition.load_partition(-1)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0 )\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0 )\n",
    "test_loader = valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() and args.use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import segmentation_models_pytorch as smp\n",
    "import numpy as np\n",
    "from poutyne import Model\n",
    "\n",
    "def test(net, testloader, steps: int = None, device: str = \"cpu\", args=None):\n",
    "    net = net.to(device)\n",
    "    \n",
    "    last_layer_name = list(net.named_children())[-1][0]\n",
    "    parameters = [\n",
    "        {'params': [p for n, p in net.named_parameters() if last_layer_name not in n], 'lr': args.learning_rate},\n",
    "        {'params': [p for n, p in net.named_parameters() if last_layer_name in n], 'lr': args.learning_rate*args.multifly_lr_lastlayer},\n",
    "    ]\n",
    "    optimizer = optim.Adam(parameters, lr=args.learning_rate)\n",
    "\n",
    "    if args.loss_fn == \"cross_entropy\":\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif args.loss_fn == \"dice_loss\":\n",
    "        criterion = smp.losses.DiceLoss(mode = 'multiclass')\n",
    "    elif args.loss_fn == \"focal_loss\":\n",
    "        criterion = smp.losses.FocalLoss(mode = 'multiclass')\n",
    "    elif args.loss_fn == \"LovaszLoss\":\n",
    "        criterion = smp.losses.LovaszLoss(mode = 'multiclass')\n",
    "    \n",
    "    from torchmetrics import Accuracy\n",
    "\n",
    "    def calculate_pixel_accuracy(predictions, targets):\n",
    "        predictions = predictions.cpu().detach().numpy()\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        \"\"\"\n",
    "        Calculates the Pixel Accuracy for Segmentation.\n",
    "        \n",
    "        Args:\n",
    "            predictions (tensor): Predicted segmentation masks.\n",
    "            targets (tensor): Ground truth segmentation masks.\n",
    "        \n",
    "        Returns:\n",
    "            pixel_accuracy (tensor): Pixel Accuracy for Segmentation.\n",
    "        \"\"\"\n",
    "        accuracy = Accuracy(num_classes=21, task=\"multiclass\")\n",
    "        pixel_accuracy = accuracy(predictions, targets)\n",
    "        return pixel_accuracy\n",
    "\n",
    "    model = Model(\n",
    "        net,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        batch_metrics=['accuracy'],\n",
    "        epoch_metrics=['f1', torchmetrics.JaccardIndex(num_classes=args.num_classes, task=\"multiclass\"),\n",
    "                       'precision', 'recall'],\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    logs = model.evaluate_generator(testloader, return_dict_format=True) #, steps=steps)\n",
    "    \n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_logs = {}\n",
    "args.loss_fn = \"cross_entropy\"\n",
    "deeplabv3plus = models.get_network(\"deeplabv3plus\", args.num_classes, args.pretrained, args.excluded_heads)\n",
    "deeplabv3plus.load_state_dict(model_state_dict[-1])\n",
    "total_logs[-1] = test(deeplabv3plus, test_loader, device=args.device, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.output_size = 56\n",
    "datasetpartition = datasets.PascalVocSegmentationPartition(args)\n",
    "train_dataset, valid_dataset = datasetpartition.load_partition(-1)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0 )\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0 )\n",
    "test_loader = valid_loader\n",
    "\n",
    "segformer = models.get_network(\"segformer\", args.num_classes, args.pretrained, args.excluded_heads)\n",
    "from tqdm import tqdm\n",
    "for index in tqdm([-2, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]):\n",
    "    segformer.load_state_dict(model_state_dict[index])\n",
    "    total_logs[index] = test(segformer, test_loader, device=args.device, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_logs_df = pd.DataFrame(total_logs)\n",
    "total_logs_df = total_logs_df.transpose()\n",
    "total_logs_df.reset_index(names='clients', inplace=True)\n",
    "replace_dict={'-1': 'global (segformer)', '-2': 'global (deeplabv3plus)', '0': 'client 0', '1': 'client 1', '2': 'client 2', '3': 'client 3', '4': 'client 4', '5': 'client 5', '6': 'client 6', '7': 'client 7', '8': 'client 8', '9': 'client 9'}\n",
    "replace_dict = {int(k): v for k, v in replace_dict.items()}\n",
    "total_logs_df['clients'] = total_logs_df.clients.apply(lambda x: replace_dict[x])\n",
    "rename_columns = {'test_loss':'loss', 'test_acc':'acc', 'test_fscore_macro':'f1', 'test_multiclass_jaccard_index':'mIOU', 'test_precision_macro':'precision', 'test_recall_macro':'recall'}\n",
    "total_logs_df.rename(columns=rename_columns, inplace=True)\n",
    "total_logs_df.to_csv(\"./total_logs_NIID.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "total_logs_df = pd.DataFrame(total_logs)\n",
    "# -2 : global (segformerb0)\n",
    "# -1 : global (resnet34)\n",
    "# 0~10 : local clients  \n",
    "\n",
    "total_logs_df.to_csv(\"./total_logs_NIID.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = load_all_models(\"/home/suncheol/code/FedTest/0_FedMHAD_Seg/checkpoints/_57584\")# 55227\")\n",
    "model_state_dict[-1] = torch.load(\"/home/suncheol/code/FedTest/0_FedMHAD_Seg/checkpoints/17751/global/model_round89_acc82.38_loss0.59.pth\")\n",
    "model_state_dict[-2] = torch.load(\"/home/suncheol/code/FedTest/0_FedMHAD_Seg/checkpoints/52840/global/model_round78_acc83.25_loss0.62.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.output_size = 224\n",
    "datasetpartition = datasets.PascalVocSegmentationPartition(args)\n",
    "train_dataset, valid_dataset = datasetpartition.load_partition(-1)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0 )\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0 )\n",
    "test_loader = valid_loader\n",
    "\n",
    "total_logs = {}\n",
    "args.loss_fn = \"cross_entropy\"\n",
    "deeplabv3plus = models.get_network(\"deeplabv3plus\", args.num_classes, args.pretrained, args.excluded_heads)\n",
    "deeplabv3plus.load_state_dict(model_state_dict[-1])\n",
    "total_logs[-1] = test(deeplabv3plus, test_loader, device=args.device, args=args)\n",
    "\n",
    "args.output_size = 56\n",
    "datasetpartition = datasets.PascalVocSegmentationPartition(args)\n",
    "train_dataset, valid_dataset = datasetpartition.load_partition(-1)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0 )\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0 )\n",
    "test_loader = valid_loader\n",
    "\n",
    "segformer = models.get_network(\"segformer\", args.num_classes, args.pretrained, args.excluded_heads)\n",
    "from tqdm import tqdm\n",
    "for index in tqdm([-2, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]):\n",
    "    segformer.load_state_dict(model_state_dict[index])\n",
    "    total_logs[index] = test(segformer, test_loader, device=args.device, args=args)\n",
    "    \n",
    "total_logs_df = pd.DataFrame(total_logs)\n",
    "total_logs_df = total_logs_df.transpose()\n",
    "total_logs_df.reset_index(names='clients', inplace=True)\n",
    "replace_dict={'-1': 'global (segformer)', '-2': 'global (deeplabv3plus)', '0': 'client 0', '1': 'client 1', '2': 'client 2', '3': 'client 3', '4': 'client 4', '5': 'client 5', '6': 'client 6', '7': 'client 7', '8': 'client 8', '9': 'client 9'}\n",
    "replace_dict = {int(k): v for k, v in replace_dict.items()}\n",
    "total_logs_df['clients'] = total_logs_df.clients.apply(lambda x: replace_dict[x])\n",
    "rename_columns = {'test_loss':'loss', 'test_acc':'acc', 'test_fscore_macro':'f1', 'test_multiclass_jaccard_index':'mIOU', 'test_precision_macro':'precision', 'test_recall_macro':'recall'}\n",
    "total_logs_df.rename(columns=rename_columns, inplace=True)\n",
    "total_logs_df.to_csv(\"./total_logs_IID.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def plot_bargraph(df, save_path, title):\n",
    "    \"\"\"\n",
    "    :param df: DataFrame with columns 'clients', 'mIOU', 'F1', 'Acc'\n",
    "    \"\"\"\n",
    "    # 데이터를 긴 형식으로 변경합니다.\n",
    "    df_long = pd.melt(df, id_vars=['clients'], value_vars=df.columns[1:])\n",
    "    sns.color_palette(\"Set2\")\n",
    "    # 바 그래프 그리기\n",
    "    plt.figure(figsize=(20, 7))\n",
    "    ax = sns.barplot(x='clients', y='value', hue='variable', data=df_long, palette=\"deep\")\n",
    "    # with labels \n",
    "    for p in ax.patches:\n",
    "        ax.annotate(\"%.2f\" % p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "            ha='center', va='center', fontsize=11, xytext=(0, 20),\n",
    "            textcoords='offset points')\n",
    "    ax.set_ylim(0.2, 0.9)\n",
    "    \n",
    "    # draw text percentile of FL-Segformer vs FL-Deeplabv3+(Rn34) performance\n",
    "    \n",
    "    plt.grid(axis='y', linestyle='-', alpha=0.7)\n",
    "    \n",
    "    # x축과 y축 라벨 설정\n",
    "    plt.xlabel('')\n",
    "    \n",
    "    # x축 font size \n",
    "    plt.xticks(fontsize=12)\n",
    "    \n",
    "    plt.ylabel('Scores', fontsize=15)\n",
    "    #plt.xticks(rotation=20, ha='right', fontsize=12)\n",
    "    \n",
    "    # draw grid lines\n",
    "    plt.grid(axis='y', linestyle='-', alpha=0.4)\n",
    "    # 그래프 제목 설정\n",
    "    plt.title(title, fontsize=18)\n",
    "    \n",
    "    # 범례 추가\n",
    "    plt.legend(title='Metrics (Test)', loc='lower right', fontsize=15, title_fontsize=15)\n",
    "    # 그래프 표시\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_logs_df = pd.read_csv(\"./total_logs_IID.csv\")\n",
    "total_logs_df.acc = total_logs_df.acc / 100\n",
    "total_logs_df = total_logs_df[['clients', 'acc', 'f1', 'precision', 'recall']]\n",
    "plot_bargraph(total_logs_df, './total_logs_IID.png', 'Segmentation Performance on IID Dataset')\n",
    " \n",
    "total_logs_df = pd.read_csv(\"./total_logs_NIID.csv\")\n",
    "total_logs_df.acc = total_logs_df.acc / 100\n",
    "total_logs_df = total_logs_df[['clients', 'acc', 'f1', 'precision', 'recall']]\n",
    "plot_bargraph(total_logs_df, './total_logs_NIID.png', 'Segmentation Performance on NIID Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
