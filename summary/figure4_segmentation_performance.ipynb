{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 각각의 Client로컬 학습 모델의 Receptive Field를 계산한다.\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "import torch\n",
    "\n",
    "def load_all_models(path):\n",
    "    folder_path = path\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pth\"):\n",
    "                file_list.append(pathlib.Path(root) / file)\n",
    "    # sorting file_list\n",
    "    file_list.sort()\n",
    "    # dirnames\n",
    "    model_state_dict = {}\n",
    "    for file in file_list:\n",
    "        print(\"file_name : \", file)\n",
    "        # get index from file name ex. \"client_0_best_models\" > \"0\", \"test 1\" > \"1\"\n",
    "        if 'global' in file.parent.name:\n",
    "            model_state_dict[-1] = torch.load(file) \n",
    "            print(\"file.parent.name: \", file.parent.name)\n",
    "        else:\n",
    "            index = int(file.parent.name.split('_')[1])\n",
    "            model_state_dict[index] = torch.load(file)\n",
    "            print(\"file.parent.name: \", file.parent.name, \"index: \", index)\n",
    "    return model_state_dict\n",
    "\n",
    "model_state_dict = {}\n",
    "model_state_dict[-1] = torch.load(\"/home/suncheol/code/FedTest/0_FedMHAD_Seg/checkpoints/59831/global/model_round15_acc83.90_loss0.69.pth\")\n",
    "model_state_dict[-2] = torch.load(\"/home/suncheol/code/FedTest/0_FedMHAD_Seg/checkpoints/17751/global/model_round89_acc82.38_loss0.59.pth\")\n",
    "# model_state_dict[-1] = torch.load(\"/home/suncheol/code/FedTest/0_FedMHAD_Seg/checkpoints/42705/client_3_best_models/model_round88_acc88.08_loss0.36.pth\")\n",
    "# model_state_dict.update(load_all_models(\"/home/suncheol/code/FedTest/0_FedMHAD_Seg/checkpoints/55227\")) # iid\n",
    "model_state_dict.update(load_all_models(\"/home/suncheol/code/FedTest/0_FedMHAD_Seg/checkpoints/56121\")) # non-iid24211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import models\n",
    "import utils\n",
    "import datasets\n",
    "import config\n",
    "utils.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = config.init_args(server=True, ipykernel=True)\n",
    "args.batch_size = 20\n",
    "args.output_size = 224\n",
    "datasetpartition_224 = datasets.PascalVocSegmentationPartition(args)\n",
    "train_dataset_224, valid_dataset_224 = datasetpartition_224.load_partition(-1)\n",
    "valid_loader_224 = DataLoader(valid_dataset_224, batch_size=args.batch_size, shuffle=False, num_workers=0 )\n",
    "test_loader_224 = valid_loader_224\n",
    "img_224, lab_224 = next(iter(test_loader_224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.output_size = 56\n",
    "datasetpartition = datasets.PascalVocSegmentationPartition(args)\n",
    "train_dataset, valid_dataset = datasetpartition.load_partition(-1)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=False, num_workers=0 )\n",
    "test_loader = valid_loader\n",
    "img, lab = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.get_network(\"segformer\", 21, pretrained=True).cuda()\n",
    "network2 = models.get_network(\"deeplabv3plus\", 21, pretrained=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # img_224 vs img plot\n",
    "# %matplotlib inline\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# ax[0].imshow(img_224[0].permute(1, 2, 0))\n",
    "# ax[1].imshow(img[0].permute(1, 2, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape, lab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) draw orginal image\n",
    "# 2) draw predicted image with ground truth boundary for each client\n",
    "# CUDA_LAUNCH_BLOCKING=1.\n",
    "\n",
    "img = img.cuda()\n",
    "# img_index = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
    "img_index = [0,4,7,9,11,16]\n",
    "img = img[img_index]\n",
    "lab = lab[img_index]\n",
    "network = network.cuda()\n",
    "network2 = network2.cuda()\n",
    "preds = {}\n",
    "for key in model_state_dict.keys():\n",
    "    if key == -2:\n",
    "        network2.load_state_dict(model_state_dict[key])\n",
    "        out = network2(img).detach().cpu().numpy()\n",
    "        # resize to (56, 56)\n",
    "        # b x c x h x w > b x c x 56 x 56\n",
    "        out = torch.tensor(out)\n",
    "        out = torch.nn.functional.interpolate(out, size=(56, 56), mode='bilinear', align_corners=False)\n",
    "        out = out.detach().cpu().numpy()\n",
    "        preds[key] = out\n",
    "    else:\n",
    "        network.load_state_dict(model_state_dict[key])\n",
    "        preds[key] = network(img).detach().cpu().numpy()\n",
    "img = img.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mIOUscores = []\n",
    "def compute_mIoU(pred, target):\n",
    "    ious = []\n",
    "    for cls in range(21):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        print(\"pred_inds: \", pred_inds.shape, \"target_inds: \", target_inds.shape)\n",
    "        intersection = pred_inds[target_inds].sum()\n",
    "        union = pred_inds.sum() + target_inds.sum() - intersection\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))  # if there is no ground truth, do not include in evaluation\n",
    "        else:\n",
    "            ious.append(float(intersection) / max(union, 1))\n",
    "    return np.nanmean(ious)\n",
    "\n",
    "for i in model_state_dict.keys():\n",
    "    mIOUscores = []\n",
    "    temp_pred = preds[i]\n",
    "    temp_pred = np.argmax(temp_pred, axis=1)\n",
    "    for j in range(len(img_index)):\n",
    "        mIOUscores.append(compute_mIoU(temp_pred[j], lab[j].numpy()))\n",
    "    total_mIOUscores.append(mIOUscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row = len(img)\n",
    "n_col = len(model_state_dict.keys()) + 2 # 1 for original image, 1 for ground truth\n",
    "\n",
    "pascal_voc_labels = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "def get_label_text(label):\n",
    "    unique_labels = np.unique(label)\n",
    "    num_instances = len(unique_labels)\n",
    "    label_text = [pascal_voc_labels[l] for l in unique_labels if l != 0]\n",
    "    label_text = \", \".join(label_text)\n",
    "    return label_text\n",
    "\n",
    "fig = plt.figure(figsize=(n_col*2, n_row*2))\n",
    "for i in range(n_row):\n",
    "    plt.subplot(n_row, n_col, i*n_col+1)\n",
    "    if i == 0:\n",
    "        plt.title(\"Original\", fontsize=15)\n",
    "    plt.imshow(img[i].permute(1, 2, 0))\n",
    "    plt.subplot(n_row, n_col, i*n_col+2)\n",
    "    if i == 0:\n",
    "        plt.title(\"Ground Truth\", fontsize=15)\n",
    "    plt.imshow(lab[i])\n",
    "    for j, key in enumerate(model_state_dict.keys()):\n",
    "        plt.subplot(n_row, n_col, i*n_col+3+j)\n",
    "        if i == 0:\n",
    "            client_name  = f\"Client {key}\"\n",
    "            client_name  = client_name if key != -2 else \"FL-DL+-Rn34\"\n",
    "            client_name  = client_name if key != -1 else \"FL-Segformer\"\n",
    "            plt.title(client_name, fontsize=15)\n",
    "        plt.imshow(np.argmax(preds[key][i], axis=0))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.text(0.95, 0.95, f\"mIoU: {total_mIOUscores[j][i]:.2f}\", color=\"white\", fontsize=14, bbox=dict(facecolor='black', alpha=0.5)\n",
    "                 , verticalalignment='top', horizontalalignment='right', transform=plt.gca().transAxes)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.suptitle(\"Segmentation predictions of Each Client\", fontsize=20, y=1.05)\n",
    "plt.savefig(\"figure4.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_row = len(img)\n",
    "n_col = len(model_state_dict.keys()) + 2  # 1 for original image, 1 for ground truth\n",
    "pascal_voc_labels = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "def get_label_text(label):\n",
    "    unique_labels = np.unique(label)\n",
    "    num_instances = len(unique_labels)\n",
    "    label_text = [pascal_voc_labels[l] for l in unique_labels if l != 0]\n",
    "    label_text = \", \".join(label_text)\n",
    "    return label_text\n",
    "\n",
    "fig = plt.figure(figsize=(n_col*2, n_row*2))\n",
    "fig.suptitle(\"Segmentation predictions of Each Client\", fontsize=20)\n",
    "\n",
    "for i in range(n_row):\n",
    "    plt.subplot(n_row, n_col, i*n_col+1)\n",
    "    if i == 0:\n",
    "        plt.title(\"Original\", fontsize=15)\n",
    "    plt.imshow(img[i].permute(1, 2, 0))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    label_text = get_label_text(lab[i])\n",
    "    print(label_text)\n",
    "    plt.ylabel(get_label_text(lab[i]), fontsize=12, rotation=0, labelpad=40)  # y축 레이블 추가\n",
    "    plt.subplot(n_row, n_col, i*n_col+2)\n",
    "    if i == 0:\n",
    "        plt.title(\"Ground Truth\", fontsize=15)\n",
    "    plt.imshow(lab[i])\n",
    "    plt.text(0.95, 0.95, f\"{label_text}\", color=\"white\", fontsize=14, bbox=dict(facecolor='black', alpha=0.5)\n",
    "                 , verticalalignment='top', horizontalalignment='right', transform=plt.gca().transAxes)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    for j, key in enumerate(model_state_dict.keys()):\n",
    "        plt.subplot(n_row, n_col, i*n_col+3+j)\n",
    "        \n",
    "        client_name  = f\"Client {key}\"\n",
    "        client_name  = client_name if key != -2 else \"FL-DL+-Rn34\"\n",
    "        client_name  = client_name if key != -1 else \"FL-Segformer\"\n",
    "        plt.title(client_name, fontsize=15)\n",
    "        plt.imshow(np.argmax(preds[key][i], axis=0))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.text(0.95, 0.95, f\"mIoU: {total_mIOUscores[j][i]:.2f}\", color=\"white\", fontsize=14, bbox=dict(facecolor='black', alpha=0.5)\n",
    "                 , verticalalignment='top', horizontalalignment='right', transform=plt.gca().transAxes)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "#figure title\n",
    "plt.savefig(\"figure4.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
